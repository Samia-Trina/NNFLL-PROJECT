{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3081988,"sourceType":"datasetVersion","datasetId":1864860}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:42:33.815376Z","iopub.execute_input":"2025-03-13T18:42:33.815700Z","iopub.status.idle":"2025-03-13T18:43:19.886577Z","shell.execute_reply.started":"2025-03-13T18:42:33.815672Z","shell.execute_reply":"2025-03-13T18:43:19.885136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport requests\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndataset_path = '/kaggle/input/stanford-image-paragraph-captioning-dataset/stanford_df_rectified.csv'\nimage_folder_path = '/kaggle/input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images'\n\ndataset = pd.read_csv(dataset_path)\ndataset = dataset.sample(2000, random_state=42).reset_index(drop=True)\nprint(f\"Reduced dataset size: {len(dataset)} rows\")\nprint(dataset.head())\n\nimage_files = [f for f in os.listdir(image_folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\nprint(f\"Found {len(image_files)} images in {image_folder_path}\")\nprint(\"Sample images:\", image_files[:5])\n\nnew_image_folder = '/kaggle/working/downloaded_images'\nos.makedirs(new_image_folder, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:47:36.327218Z","iopub.execute_input":"2025-03-13T18:47:36.327617Z","iopub.status.idle":"2025-03-13T18:47:38.426116Z","shell.execute_reply.started":"2025-03-13T18:47:36.327590Z","shell.execute_reply":"2025-03-13T18:47:38.424838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def download_image(url, filename, output_folder):\n    try:\n        response = requests.get(url, timeout=10)\n        if response.status_code == 200:\n            image_path = os.path.join(output_folder, filename)\n            with open(image_path, 'wb') as f:\n                f.write(response.content)\n            Image.open(image_path).convert(\"RGB\")\n            return True\n        else:\n            print(f\"Failed to download {url} (status: {response.status_code})\")\n            return False\n    except Exception as e:\n        print(f\"Error downloading {url}: {e}\")\n        return False\n\nsuccessful_images = []\ndataset['image_filename'] = dataset['Image_name'].apply(lambda x: f\"{x}.jpg\")\n\nfor i, row in dataset.iterrows():\n    filename = row['image_filename']\n    original_path = os.path.join(image_folder_path, filename)\n    new_path = os.path.join(new_image_folder, filename)\n    \n    if os.path.exists(original_path):\n        try:\n            Image.open(original_path).convert(\"RGB\")\n            os.system(f\"cp {original_path} {new_path}\")\n            successful_images.append(filename)\n        except Exception as e:\n            print(f\"Error copying {filename}: {e}\")\n            url = row['url']\n            if download_image(url, filename, new_image_folder):\n                successful_images.append(filename)\n    else:\n        print(f\"Image {filename} not found, downloading...\")\n        url = row['url']\n        if download_image(url, filename, new_image_folder):\n            successful_images.append(filename)\n\nprint(f\"Successfully processed {len(successful_images)} images\")\ndataset = dataset[dataset['image_filename'].isin(successful_images)].reset_index(drop=True)\nprint(f\"Final dataset size after filtering: {len(dataset)} rows\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:48:24.186813Z","iopub.execute_input":"2025-03-13T18:48:24.187342Z","iopub.status.idle":"2025-03-13T18:49:02.080644Z","shell.execute_reply.started":"2025-03-13T18:48:24.187309Z","shell.execute_reply":"2025-03-13T18:49:02.079389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch\nfrom collections import Counter\nimport nltk\nnltk.download('punkt')\n\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # EfficientNet-B0 default size\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndef clean_text(text):\n    return text.lower().strip()\n\ndataset['Paragraph'] = dataset['Paragraph'].apply(clean_text)\n\nall_captions = dataset['Paragraph'].tolist()\ntokenizer = nltk.word_tokenize\nword_counts = Counter()\nfor caption in all_captions:\n    word_counts.update(tokenizer(caption))\n\nvocab = ['<PAD>', '<START>', '<END>', '<UNK>'] + [word for word, count in word_counts.items() if count >= 3]\nword2idx = {word: idx for idx, word in enumerate(vocab)}\nidx2word = {idx: word for word, idx in word2idx.items()}\nvocab_size = len(vocab)\nprint(f\"Vocabulary size: {vocab_size}\")\n\ndef caption_to_sequence(caption):\n    tokens = tokenizer(caption)\n    seq = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n    return [word2idx['<START>']] + seq + [word2idx['<END>']]\n\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, dataframe, image_folder, transform=None):\n        self.dataframe = dataframe\n        self.image_folder = image_folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = os.path.join(self.image_folder, row['image_filename'])\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n            caption = row['Paragraph']\n            if self.transform:\n                image = self.transform(image)\n            caption_seq = caption_to_sequence(caption)\n            return image, torch.tensor(caption_seq)\n        except Exception as e:\n            print(f\"Error loading {image_path}: {e}\")\n            return None, None\n\ntrain_data = dataset[dataset['train'] == 1]\nval_data = dataset[dataset['val'] == 1]\ntest_data = dataset[dataset['test'] == 1]\n\nbatch_size = 8\ntrain_dataset = ImageCaptionDataset(train_data, new_image_folder, image_transform)\nval_dataset = ImageCaptionDataset(val_data, new_image_folder, image_transform)\ntest_dataset = ImageCaptionDataset(test_data, new_image_folder, image_transform)\n\ndef collate_fn(batch):\n    batch = [b for b in batch if b[0] is not None]\n    if len(batch) == 0:\n        return None\n    images, captions = zip(*batch)\n    images = torch.stack(images)\n    captions = torch.nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=word2idx['<PAD>'])\n    return images, captions\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:49:30.858832Z","iopub.execute_input":"2025-03-13T18:49:30.859304Z","iopub.status.idle":"2025-03-13T18:49:40.421595Z","shell.execute_reply.started":"2025-03-13T18:49:30.859273Z","shell.execute_reply":"2025-03-13T18:49:40.420586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision.models import efficientnet_b0\n\nclass CNNLSTMModel(nn.Module):\n    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=1):\n        super(CNNLSTMModel, self).__init__()\n        self.cnn = efficientnet_b0(pretrained=True)\n        self.cnn.classifier = nn.Identity()  # Remove final classifier layer\n        # Freeze CNN initially\n        for param in self.cnn.parameters():\n            param.requires_grad = False\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        # EfficientNet-B0 outputs 1280 features\n        self.lstm = nn.LSTM(embed_size + 1280, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, images, captions):\n        features = self.cnn(images)  # [batch_size, 1280]\n        features = features.unsqueeze(1).repeat(1, captions.size(1), 1)  # [batch_size, seq_len, 1280]\n        embeddings = self.embed(captions)  # [batch_size, seq_len, embed_size]\n        inputs = torch.cat((features, embeddings), dim=2)  # [batch_size, seq_len, 1280 + embed_size]\n        outputs, _ = self.lstm(inputs)  # [batch_size, seq_len, hidden_size]\n        outputs = self.fc(outputs)  # [batch_size, seq_len, vocab_size]\n        return outputs\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNNLSTMModel(vocab_size).to(device)\nprint(\"Model loaded on:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:50:11.145623Z","iopub.execute_input":"2025-03-13T18:50:11.146285Z","iopub.status.idle":"2025-03-13T18:50:11.904375Z","shell.execute_reply.started":"2025-03-13T18:50:11.146250Z","shell.execute_reply":"2025-03-13T18:50:11.902924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.optim.lr_scheduler import StepLR\n\ncriterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nnum_epochs = 10\noutput_dir = '/kaggle/working/checkpoints'\nos.makedirs(output_dir, exist_ok=True)\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n        if batch is None:\n            continue\n        images, captions = batch\n        images, captions = images.to(device), captions.to(device)\n        \n        outputs = model(images, captions[:, :-1])\n        loss = criterion(outputs.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n        train_loss += loss.item()\n        \n        preds = outputs.argmax(dim=2)\n        mask = captions[:, 1:] != word2idx['<PAD>']\n        train_correct += (preds[mask] == captions[:, 1:][mask]).sum().item()\n        train_total += mask.sum().item()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.cuda.empty_cache()\n\n    avg_train_loss = train_loss / len(train_loader)\n    train_accuracy = train_correct / train_total if train_total > 0 else 0\n    train_losses.append(avg_train_loss)\n    train_accuracies.append(train_accuracy)\n    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n\n    torch.save(model.state_dict(), os.path.join(output_dir, f'model_epoch_{epoch+1}.pt'))\n\n    model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Validation\"):\n            if batch is None:\n                continue\n            images, captions = batch\n            images, captions = images.to(device), captions.to(device)\n            outputs = model(images, captions[:, :-1])\n            loss = criterion(outputs.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n            val_loss += loss.item()\n            \n            preds = outputs.argmax(dim=2)\n            mask = captions[:, 1:] != word2idx['<PAD>']\n            val_correct += (preds[mask] == captions[:, 1:][mask]).sum().item()\n            val_total += mask.sum().item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = val_correct / val_total if val_total > 0 else 0\n    val_losses.append(avg_val_loss)\n    val_accuracies.append(val_accuracy)\n    print(f\"Epoch {epoch+1}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n    scheduler.step()\n    if epoch > 1 and val_losses[-1] > val_losses[-2]:\n        print(\"Early stopping triggered\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:50:41.289264Z","iopub.execute_input":"2025-03-13T18:50:41.289745Z","iopub.status.idle":"2025-03-13T19:15:54.413464Z","shell.execute_reply.started":"2025-03-13T18:50:41.289704Z","shell.execute_reply":"2025-03-13T19:15:54.412155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\nplt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Val Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Curve')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/loss_accuracy_curves.png')\nplt.show()\nprint(\"Curves saved to /kaggle/working/loss_accuracy_curves.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T19:16:00.789693Z","iopub.execute_input":"2025-03-13T19:16:00.790073Z","iopub.status.idle":"2025-03-13T19:16:01.704647Z","shell.execute_reply.started":"2025-03-13T19:16:00.790042Z","shell.execute_reply":"2025-03-13T19:16:01.703300Z"}},"outputs":[],"execution_count":null}]}